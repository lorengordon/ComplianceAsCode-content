prodtype: rhel7,rhel8
description: 'Remove any <tt>robots.txt</tt> files that may exist with any web content.

    Other methods must be employed if there is information on the web site that

    needs protection from search engines and public view. Inspect all instances of

    <tt>DocumentRoot</tt> and <tt>Alias</tt> and remove any <tt>robots.txt</tt> file.

    <pre>$ sudo rm -f path/to/robots.txt</pre>'
rationale: 'Search engines are constantly at work on the Internet. Search engines
    are

    augmented by agents, often referred to as spiders or bots, which endeavor to

    capture and catalog web-site content. In turn, these search engines make the

    content they obtain and catalog available to any public web user.

    <br /><br />

    To request

    that a well behaved search engine not crawl and catalog a site, the web site may

    contain a file called robots.txt. This file contains directories and files that

    the web server SA desires not be crawled or cataloged, but this file can also
    be

    used, by an attacker or poorly coded search engine, as a directory and file

    index to a site. This information may be used to reduce an attacker''s time

    searching and traversing the web site to find files that might be relevant. If

    information on the web site needs to be protected from search engines and public

    view, other methods must be used.'
severity: medium
references:
    stigid: WG310
identifiers: {}
ocil_clause: it is not
ocil: 'Inspect all instances of <tt>DocumentRoot</tt> and <tt>Alias</tt>. No

    <tt>robots.txt</tt> file should exist.'
oval_external_content: null
fixtext: ''
checktext: ''
vuldiscussion: ''
srg_requirement: ''
warnings: []
conflicts: []
requires: []
policy_specific_content: {}
platform: null
platforms: !!set {}
sce_metadata: {}
inherited_platforms: !!set {}
cpe_platform_names: !!set {}
inherited_cpe_platform_names: !!set {}
bash_conditional: null
fixes: {}
title: The robots.txt Files Must Not Exist
definition_location: /home/oscap/content/linux_os/guide/services/http/securing_httpd/httpd_secure_content/httpd_remove_robots_file/rule.yml
template: null
documentation_complete: true
